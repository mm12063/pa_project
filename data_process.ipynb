{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72ad6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d06c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_stocks = {\n",
    "    \"albemarle\": \"alb\",\n",
    "    \"ganfeng\": \"gnenf\",\n",
    "    \"livent\": \"lthm\",\n",
    "    \"lithium americas\": \"lac\",\n",
    "    \"lg chem\": \"051910\",\n",
    "    \"toshiba corp\": \"tosyy\",\n",
    "    \"panasonic\": \"pcrfy\",\n",
    "    \"samsung\": \"005930\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879022b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timestamps = []\n",
    "all_headlines = []\n",
    "all_tickers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "039be7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"NewsData\", \"Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94243af",
   "metadata": {},
   "source": [
    "### Process dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee28d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(all_tickers))\n",
    "\n",
    "FILENAME = \"analyst_ratings_processed.csv\"\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR, FILENAME))\n",
    "\n",
    "TIMESTAMP_HEADER = 'date'\n",
    "HEADLINE_HEADER = 'title'\n",
    "TICKER_HEADER = 'stock'\n",
    "\n",
    "dataset[HEADLINE_HEADER] = dataset[HEADLINE_HEADER].str.lower()\n",
    "dataset[TICKER_HEADER] = dataset[TICKER_HEADER].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf6dc95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stock_name, stock_ticker in required_stocks.items():\n",
    "    stock_df = dataset[dataset['title'].str.contains(stock_name) | dataset['stock'].str.contains(stock_ticker)]\n",
    "\n",
    "    all_timestamps.extend(stock_df[TIMESTAMP_HEADER].tolist())\n",
    "    all_headlines.extend(stock_df[HEADLINE_HEADER].tolist())\n",
    "    all_tickers.extend(stock_df[TICKER_HEADER].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f6fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345 2345 2345\n"
     ]
    }
   ],
   "source": [
    "print(len(all_timestamps), len(all_headlines), len(all_tickers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609e4b5",
   "metadata": {},
   "source": [
    "### Process dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee720adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2345\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(all_tickers))\n",
    "\n",
    "FILENAME = \"abcnews-date-text.csv\"\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR, FILENAME))\n",
    "\n",
    "TIMESTAMP_HEADER = 'publish_date'\n",
    "HEADLINE_HEADER = 'headline_text'\n",
    "\n",
    "dataset[HEADLINE_HEADER] = dataset[HEADLINE_HEADER].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f303656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_name, stock_ticker in required_stocks.items():\n",
    "    stock_df = dataset[\n",
    "        dataset[HEADLINE_HEADER].str.contains(stock_name)\n",
    "        | dataset[HEADLINE_HEADER].str.contains(\" \" + stock_ticker)\n",
    "        | dataset[HEADLINE_HEADER].str.contains(stock_ticker + \" \")]\n",
    "\n",
    "    all_timestamps.extend(stock_df[TIMESTAMP_HEADER].tolist())\n",
    "    all_headlines.extend(stock_df[HEADLINE_HEADER].tolist())\n",
    "    all_tickers.extend([stock_ticker]*stock_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf2597",
   "metadata": {},
   "source": [
    "### Process dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7230fc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5447\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(all_tickers))\n",
    "\n",
    "FILENAME = \"RedditNews.csv\"\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR, FILENAME))\n",
    "\n",
    "TIMESTAMP_HEADER = 'Date'\n",
    "HEADLINE_HEADER = 'News'\n",
    "\n",
    "dataset[HEADLINE_HEADER] = dataset[HEADLINE_HEADER].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4947de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_name, stock_ticker in required_stocks.items():\n",
    "    stock_df = dataset[\n",
    "        dataset[HEADLINE_HEADER].str.contains(stock_name)\n",
    "        | dataset[HEADLINE_HEADER].str.contains(\" \" + stock_ticker)\n",
    "        | dataset[HEADLINE_HEADER].str.contains(stock_ticker + \" \")]\n",
    "\n",
    "    all_timestamps.extend(stock_df[TIMESTAMP_HEADER].tolist())\n",
    "    all_headlines.extend(stock_df[HEADLINE_HEADER].tolist())\n",
    "    all_tickers.extend([stock_ticker]*stock_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303908c7",
   "metadata": {},
   "source": [
    "### Process dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8131105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5727\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(all_tickers))\n",
    "\n",
    "FILENAME = \"us_equities_news_dataset.csv\"\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR, FILENAME))\n",
    "\n",
    "TIMESTAMP_HEADER = 'release_date'\n",
    "HEADLINE_HEADER = 'title'\n",
    "TICKER_HEADER = 'ticker'\n",
    "\n",
    "dataset[HEADLINE_HEADER] = dataset[HEADLINE_HEADER].str.lower()\n",
    "dataset[TICKER_HEADER] = dataset[TICKER_HEADER].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e93bee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_name, stock_ticker in required_stocks.items():\n",
    "    stock_df = dataset[dataset[HEADLINE_HEADER].str.contains(stock_name) | dataset[TICKER_HEADER].str.contains(stock_ticker)]\n",
    "\n",
    "    all_timestamps.extend(stock_df[TIMESTAMP_HEADER].tolist())\n",
    "    all_headlines.extend(stock_df[HEADLINE_HEADER].tolist())\n",
    "    all_tickers.extend(stock_df[TICKER_HEADER].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4eede6",
   "metadata": {},
   "source": [
    "### Process dataset 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffaad3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6471\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(all_tickers))\n",
    "\n",
    "FILENAME = \"raw_partner_headlines.csv\"\n",
    "dataset = pd.read_csv(os.path.join(DATA_DIR, FILENAME))\n",
    "\n",
    "TIMESTAMP_HEADER = 'date'\n",
    "HEADLINE_HEADER = 'headline'\n",
    "TICKER_HEADER = 'stock'\n",
    "\n",
    "dataset[HEADLINE_HEADER] = dataset[HEADLINE_HEADER].str.lower()\n",
    "dataset[TICKER_HEADER] = dataset[TICKER_HEADER].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a79b347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_name, stock_ticker in required_stocks.items():\n",
    "    stock_df = dataset[dataset[HEADLINE_HEADER].str.contains(stock_name) | dataset[TICKER_HEADER].str.contains(stock_ticker)]\n",
    "\n",
    "    all_timestamps.extend(stock_df[TIMESTAMP_HEADER].tolist())\n",
    "    all_headlines.extend(stock_df[HEADLINE_HEADER].tolist())\n",
    "    all_tickers.extend(stock_df[TICKER_HEADER].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcb053",
   "metadata": {},
   "source": [
    "### Create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3584bcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication (9917, 3)\n",
      "After deduplication (9858, 3)\n"
     ]
    }
   ],
   "source": [
    "INDEX_HEADER = 'idx'\n",
    "SOURCE_HEADER = 'source'\n",
    "\n",
    "final_df = pd.DataFrame(list(zip(all_timestamps, all_headlines, all_tickers)), columns =[TIMESTAMP_HEADER, HEADLINE_HEADER, TICKER_HEADER])\n",
    "print(\"Before deduplication\", final_df.shape)\n",
    "# Remove duplicate rows\n",
    "final_df = final_df.drop_duplicates()\n",
    "print(\"After deduplication\", final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdcf8804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-03 12:15:00-04:00</td>\n",
       "      <td>stocks that made new 52-wk lows today include:...</td>\n",
       "      <td>ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-29 07:15:00-04:00</td>\n",
       "      <td>34 stocks moving in friday's pre-market session</td>\n",
       "      <td>alb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-28 10:06:00-04:00</td>\n",
       "      <td>deutsche bank maintains hold on albemarle, rai...</td>\n",
       "      <td>alb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-26 10:42:00-04:00</td>\n",
       "      <td>shares of several basic material companies are...</td>\n",
       "      <td>alb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-20 11:54:00-04:00</td>\n",
       "      <td>shares of several basic materials companies ar...</td>\n",
       "      <td>alb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2018-04-03 12:15:00-04:00   \n",
       "1  2020-05-29 07:15:00-04:00   \n",
       "2  2020-05-28 10:06:00-04:00   \n",
       "3  2020-05-26 10:42:00-04:00   \n",
       "4  2020-05-20 11:54:00-04:00   \n",
       "\n",
       "                                            headline stock  \n",
       "0  stocks that made new 52-wk lows today include:...   ads  \n",
       "1    34 stocks moving in friday's pre-market session   alb  \n",
       "2  deutsche bank maintains hold on albemarle, rai...   alb  \n",
       "3  shares of several basic material companies are...   alb  \n",
       "4  shares of several basic materials companies ar...   alb  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c476bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
